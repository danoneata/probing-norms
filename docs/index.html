<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Seeing what tastes good</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-4Q6Gf2aSP4eDXB8Miphtr37CMZZQ5oXLH2yaXMJ2w8e2ZtHTl7GptT4jmndRuHDT" crossorigin="anonymous">
    <link href=" https://cdn.jsdelivr.net/npm/bootstrap-icons@1.13.1/font/bootstrap-icons.min.css " rel="stylesheet">
    <link rel="icon" type="image/png" href="favicon.png">
</head>

<body>
    <div class="row text-bg-light" data-bs-theme="light">
        <div class="container mt-4" style="max-width: 720px">
            <h3 class="text-center">Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the
                Billion Parameter Era</h3>
            <div class="row">
                <div class="col-sm"></div>
                <div class="col-sm-3 text-center">
                    <a href="https://doneata.bitbucket.io/">Dan Oneață</a>
                    <p class="font-weight-light text-secondary"><span
                            style="font-variant: small-caps">politehnica</span> Bucharest</p>
                </div>
                <div class="col-sm-3 text-center">
                    <a href="https://elliottd.github.io/">Desmond Elliott</a>
                    <p class="font-weight-light text-secondary">University of Copenhagen</p>
                </div>
                <div class="col-sm-3 text-center">
                    <a href="https://scfrank.github.io/">Stella Frank</a>
                    <p class="font-weight-light text-secondary">Pioneer Center for Artificial Intelligence</p>
                </div>
                <div class="col-sm"></div>
            </div>
        </div>
    </div>

    <div class="container">
        <div class="row mt-3">
            <span class="text-center">
                <span class="mx-2">
                    <i class="bi bi-file-earmark-fill"></i> <a href="https://arxiv.org/abs/2506.03994">paper</a>
                </span>
                ·
                <span class="mx-2">
                    <i class="bi bi-github"></i> <a href="https://github.com/danoneata/seeing-what-tastes-good">code</a>
                </span>
                ·
                <span class="mx-2">
                    <i class="bi bi-easel3-fill"></i> <a
                        href="https://doneata.bitbucket.io/static/cvpr25-viscon-poster.pdf">poster</a>
                </span>
            </span>
        </div>

        <div class="row mt-3">
            <p class="alert alert-primary text-center lead">Self-supervised vision models are surprisingly good at
                predicting
                the semantic attributes of concepts.</p>
        </div>

        <div class="row mt-3">
            <div class="col-md-3">
                <h4 class="text-center mb-3">Overview</h4>
                <ul>
                    <li>Revisit classic work in distributional semantics (e.g.
                        <a href="https://aclanthology.org/P15-2119/">Rubinstein et al., 2015</a>;
                        <a href="https://aclanthology.org/C16-1264/">Collell and Moens, 2016</a>;
                        <a href="https://aclanthology.org/W17-2810/">Lucy and Gauthier, 2017</a>)
                        but with a new twist: larger models and denser
                        data.
                    </li>
                    <li>Evaluate how well large-scale pretrained models capture the semantic attributes of concepts;
                        for example, <span style="font-variant: small-caps">rose</span> <i>is red</i>, <i>smells
                            sweet</i>, <i>is a flower</i>.</li>
                    <li>Evaluate on data that go beyond perception, e.g. emotion, encyclopaedic, functional knowledge.
                    </li>
                </ul>
            </div>

            <div class="col-md-3">
                <h4 class="text-center mb-3">Methodology</h4>
                <img src="imgs/method.png" class="img-fluid" alt="Methodology overview"
                    style="max-width: 100%; height: auto;">
                <p>
                <ul>
                    <li>Train linear probes on frozen representations extracted from pretrained vision or language
                        models.</li>
                </ul>
                </p>
            </div>

            <div class="col-md-3">
                <h4 class="text-center mb-3">Datasets</h4>
                <img src="imgs/datasets.png" class="img-fluid" alt="Datasets overview"
                    style="max-width: 100%; height: auto;">
                <p class="mt-2">
                    Link the concepts from <span style="font-variant: small-caps">things</span> (<a
                        href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0223792">Hebart et al.,
                        2019</a>) to two sets of attributes:
                <ul>
                    <li><a href="https://link.springer.com/article/10.3758/BF03192726">McRae et al. (2005)</a>: Commonly
                        elicited attributes.</li>
                    <li><a href="https://www.tandfonline.com/doi/full/10.1080/02643294.2016.1147426">Binder et al.
                            (2016)</a>: Average ratings for a set of experiential attributes, based on
                        neurobiological mechanisms.</li>
                </ul>
                </p>
            </div>

            <div class="col-md-3">
                <h4 class="text-center mb-3">Results: Model Ranking</h4>
                <img src="imgs/ranking.png" class="img-fluid" alt="Results overview"
                    style="max-width: 100%; height: auto;">
                <p class="mt-2 text-secondary text-center">(Models at the top are better.)</p>
            </div>
        </div>

        <div class="row mt-4">
            <div class="col"></div>
            <div class="col-md-6">
                <h4 class="text-center">Interactive demo</h4>
                <div class="text-secondary">
                    Below you can visualise and interact with the results:
                    you can pick two models and a dataset,
                    and the app will display:
                    (i) a scatterplot of the performance of the two models on the selected dataset (top), and
                    (ii) the per concept predictions for a selected attribute (bottom).
                    To access the demo on a full page, visit
                    <a href="https://probing-norms-demo.streamlit.app/">this link</a>.
                    Since the demo is hosted on Streamlit Cloud, the app goes to sleep due to inactivity, so you might need to get it back up.
                    Currently, to get the app back up you need to do it from the full page (and not from the embedded app on this page).
            </div>
            </div>
            <div class="col"></div>
        </div>

        <div class="row mt-4">
            <iframe src="https://probing-norms-demo.streamlit.app?embed=true"
                style="height: 640px; width: 100%;"></iframe>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-j1CDi7MgGQ12Z7Qab0qlWQ/Qqz24Gc6BM0thvEMVjHnfYGF0rmFCozFSxQBxwHKO"
        crossorigin="anonymous"></script>
</body>

</html>