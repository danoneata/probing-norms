"""
We have found out that there were a few concepts with incorrect or missing category information (see `CONCEPTS_TO_UPDATE` list).
This error affected the context sentences generated by GPT4o.
We regenerated them using the updated concept information.
This script puts them in the standard format.
"""

import json

from probing_norms.data import DIR_LOCAL
from probing_norms.utils import read_file, read_json

CONCEPTS_TO_UPDATE = [
    "bat2",
    "baton3",
    "bracelet1",
    "camera1",
    "camera2",
    "chicken1",
    "chicken2",
    "chips",
    "clipper2",
    "crystal1",
    "crystal2",
    "film",
    "frame",
    "flip_flop",
    "hook1",
    "pepper1",
    "pepper2",
    "screen1",
    "shell3",
    "sling",
    "stove1",
    "stove2",
]

CONTEXT_TYPES = [
    ("10", "concept"),
    ("50", "50_concept"),
    ("50-constrained", "50_constrained_concept"),
]


def read_inp(context_type):
    _, type_ = context_type
    path = f"data/things/gpt4o_{type_}_context_sentences.jsonl"
    path = DIR_LOCAL / path
    path = str(path)

    def parse_line(line):
        line = json.loads(line)
        return line["id"], line

    return dict(read_file(path, parse_line))


def read_new(context_type):
    type_, _ = context_type

    def get_path(concept):
        path = f"data/things/categoryfix-sentences/update-{type_}/{concept}-{type_}-concept_sentences.jsonl"
        path = DIR_LOCAL / path
        return str(path)

    return {concept: read_json(get_path(concept)) for concept in CONCEPTS_TO_UPDATE}


def write(context_type, data):
    _, type_ = context_type
    path = f"data/things/gpt4o_{type_}_context_sentences_v2.jsonl"
    with open(path, "w") as f:
        for datum in data.values():
            f.write(json.dumps(datum))
            f.write("\n")


def do1(context_type):
    inp = read_inp(context_type)
    new = read_new(context_type)
    write(context_type, {**inp, **new})


for context_type in CONTEXT_TYPES:
    do1(context_type)
